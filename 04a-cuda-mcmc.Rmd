## CUDA Development Workflow

### The Host-Device Model

The first and most critical concept in CUDA programming is that a program is architected for two distinct processors: the **Host** (the CPU) and the **Device** (the GPU). This Host-Device model is the fundamental paradigm upon which all CUDA applications are built. In this model, the Host acts as the orchestrator of the overall workflow, managing program logic, I/O, and sequential tasks. The Device, in contrast, serves as a massively parallel computational workforce, executing specific, data-parallel tasks that have been offloaded to it by the Host.

As detailed in Section \@ref(gpu-memory), a direct consequence of this dual-processor architecture is the existence of two separate memory systems: the Host's main system memory (RAM) and the Device's onboard memory (VRAM). There is no unified memory space, and therefore the programmer is responsible for explicitly managing all data transfers between these two locations. The typical workflow involves the Host preparing data, dispatching it along with computational instructions to the Device, and later retrieving the results.

To facilitate this interaction, CUDA extends the C++ language with several keywords and a specific syntax. The most important of these are:

* `__global__`: A function specifier that declares a function, known as a **kernel**, that runs on the Device but is called from the Host.
* `<<...>>`: An execution configuration syntax, used only on the Host, to launch a kernel on the Device. It specifies the number of parallel threads to be created for the kernel's execution.

A simple "Hello, World!" program serves as a practical demonstration of these concepts.

```{cpp, cpp-hello-world-example, eval=FALSE, echo=TRUE, size='small', code.cap="A complete 'Hello, World!' program in CUDA C++, demonstrating the host orchestrating a kernel launch on the device."}
#include <stdio.h>

/*
 * Kernel: A function that runs on the device (GPU).
 * The __global__ specifier marks it as such.
 * This kernel will be executed by many threads in parallel.
 */
__global__ void hello_from_gpu()
{
  printf("Hello, World! from the GPU!\n");
}

/*
 * Host: The main function that runs on the CPU.
 * It orchestrates the program and launches kernels on the device.
 */
int main()
{
  // 1. A message from the host CPU.
  printf("Hello from the host CPU before launching the kernel.\n");

  // 2. The Host launches the kernel on the Device.
  // We launch a "grid" of 1 block, containing 4 threads.
  hello_from_gpu<<<1, 4>>>();

  // 3. Host must wait for the device to finish its work before exiting.
  // cudaDeviceSynchronize() is a barrier that pauses the host
  // until all previously launched device tasks are complete.
  cudaDeviceSynchronize();

  // 4. A final message from the host CPU.
  printf("Kernel launch finished. Hello from the host CPU again.\n");

  return 0;
}
```

When compiled with the NVIDIA CUDA Compiler (NVCC), this program's output demonstrates the core principles of the Host-Device model. The Host's `printf` statements execute sequentially and predictably. The kernel launch `hello_from_gpu<<1, 4>>()` dispatches the task to the GPU, creating four parallel threads, each of which executes the `printf` command inside the kernel, resulting in four distinct messages from the GPU. Crucially, the Host does not wait for the Device to complete its work by default. The call to `cudaDeviceSynchronize()` creates an explicit synchronization point, pausing the Host's execution until all four GPU threads have finished. Without this, the `main` function could finish and exit before the GPU had a chance to execute its task.

### Hierarchy of Threads, Blocks, and Grids

The previous example launch four parallel threads using the `<<1, 4>>` syntax, but this simple case obscures two critical questions. First, how does this model scale to the thousands or millions of threads required for large-scale statistical problems? Second, how does each individual thread, when operating as part of this massive workforce, determine which specific piece of data it is responsible for processing?

The answer lies in CUDA's hierarchical organization of threads. When a kernel is launched, the "army" of parallel threads is organized into a three-level hierarchy:

* **Grid**: The highest level of the hierarchy. A grid encompasses all the threads generated by a single kernel launch. It can be thought of as the entire workforce assigned to a given computational task.
* **Blocks**: The grid is partitioned into a one-, two-, or three-dimensional array of thread blocks. A block is a collection of threads that can in more advanced use cases, cooperate by sharing fast on-chip memory and synchronizing their execution. For our purposes, we will simply treat a block as a convenient grouping of threads.
* **Threads**: The fundamental unit of parallel execution. Each thread runs the same kernel code independently.

To enable each thread to perform a unique task, CUDA provides built-in variables that allow a thread to identify its position within this hierarchy. For a one-dimensional organization, the most important of these are:

* `threadIdx.x`: The index of the thread within its block.
* `blockIdx.x`: The index of the block within the grid.
* `blockDim.x`: The number of threads in each block.
* `gridDim.x`: The number of blocks in each grid.

Using these variables, each thread can compute a unique global index within its grid, which maps it to a specific element in a large data array. The standard formula for this calculation is:

```{cpp, eval=FALSE, echo=TRUE}
int global_idx = blockIdx.x * blockDim.x + threadIdx.x;
```

For example, a thread with `blockIdx.x = 2` and `threadIdx.x = 5`, in a grid where blocks have `blockDim.x = 8` threads, would calculate its global index as `2 * 8 + 5 = 21`.

A parallel vector addition provides a canonical example of this principle. Adding two vectors, $\mathbf{A}$ and $\mathbf{B}$, is an embarrassingly parallel problem, as each element-wise addition, $C_i = A_i + B_i$, is completely independent of all others. The kernel to perform this task leverages the global index to assign each thread to one such addition.

```{cpp, cpp-vector-add-kernel, eval=FALSE, echo=TRUE, size='small', code.cap="The CUDA C++ kernel for parallel vector addition, which uses the global thread index to assign each thread to a specific element-wise operation."}
/*
 * Kernel to add two vectors A and B into a result vector C.
 * Assumes d_A, d_B, and d_C are pointers to arrays on the GPU.
 * N is the number of elements in the vectors.
 */
__global__ void vectorAdd(const float *d_A, const float *d_B, float *d_C, 
                          int N) {

    // 1. Calculate the unique global index for this thread.
    int idx = blockIdx.x * blockDim.x + threadIdx.x;

    // 2. A crucial "bounds check". We often launch more threads than
    // needed for hardware reasons. This check ensures we do not
    // access memory outside the bounds of our arrays.
    if (idx < N) {
        // 3. Perform the addition for the element this thread is
        // responsible for.
        d_C[idx] = d_A[idx] + d_B[idx];
    }
}
```

On the host side, one must calculate the required grid dimensions to ensure that at least $N$ threads are launched to cover the entire dataset. A common practice is to choose a block size that is a multiple of 32 (the number of threads in a *warp*, as explained in Section \@ref(gpu-intro)) for hardware efficiency, and then calculate the number of blocks needed.

```{cpp, cpp-vector-add-launch, eval=FALSE, echo=TRUE, size='small', code.cap="Host-side logic for launching the vectorAdd kernel, including the calculation of the grid and block dimensions required to cover the entire dataset."}
// Define the size of our data
int N = 1024 * 1024; // Let's process over a million elements

// Define the number of threads per block.
int THREADS_PER_BLOCK = 256; // Multiple of 32 for hardware efficiency

// Calculate number of blocks needed in the grid to cover all N elements.
// This is a standard integer arithmetic trick to compute
// ceil(N / THREADS_PER_BLOCK).
int BLOCKS_PER_GRID = (N + THREADS_PER_BLOCK - 1) / THREADS_PER_BLOCK;

// Assume d_A, d_B, d_C are device pointers that have already been set up.
// Launch the kernel with our calculated grid and block dimensions.
vectorAdd<<<BLOCKS_PER_GRID, THREADS_PER_BLOCK>>>(d_A, d_B, d_C, N);

// Synchronize to make sure the kernel finishes.
cudaDeviceSynchronize();
```

This hierarchical model allows CUDA to scale from a handful of threads to millions, while the global index mechanism provides a robust way to map this massive parallelism to large datasets. While the `vectorAdd` example illustrates the concept, it is not yet a complete, functional program, as the device pointers (`d_A`, `d_B`, `d_C`) are assumed to exist. A simple, self-contained program that demonstrates the calculation of thread indices without performing any data manipulation is included in the code repository indicated in Appendix \@ref(appendix-code). Instead of `vectorAdd`, it uses a simpler `demoIndices` kernel, serving as a practical confirmation of the indexing logic discussed here.

### Managing Memory Between Host and Device

We have designed a `vectorAdd` kernel that is, in principle, capable of adding two large vectors. However, that kernel cannot yet function because it has no data to operate on. As a consequence of the Host and the Device having their own distinct memory systems (system RAM and GPU VRAM, respectively), data does not implicitly flow between them. To make our program functional, we must learn to explicitly manage this **data pipeline**.

This process involves three fundamental steps: allocating memory on the Device, copying data from the Host to the Device, and, after computation, copying the results back from the Device to the Host. CUDA provides a specific set of functions, which mirror standard C memory management, to control this pipeline.

* `cudaMalloc(void **devPtr, size_t size)`: The device equivalent of `malloc()`. It allocates `size` bytes of memory in the GPU's global memory. `devptr` is a pointer to a device pointer, which will hold the address of the newly allocated memory.
* `cudaMemcpy(void *dst, const void *src, size_t count, cudaMemcpyKind kind)`: The workhorse function for moving data. It copies `count` bytes of data from a source `src` to a destination `dst`. The `kind` parameter specifies the direction of the transfer, most commonly `cudaMemcpyHostToDevice` or `cudaMemcpyDeviceToHost`.
* `cudaFree(void *devPtr)`: The device equivalent to `free()`. It deallocates a block of GPU memory that was previously allocated with `cudaMalloc`. If not used appropriately, memory leaks may occur.

With these functions, we can now construct the complete, end-to-end `vectorAdd` program. We will examine its implementation step-by-step, following the typical lifecycle of a CUDA application.

#### Host-Side Setup

First, on the Host, we define the problem size and allocate standard system memory for our input vectors, `h_A` and `h_B` (prefix `h_` for host), and our result vector, `h_C`. We then initialize the input vectors.

```{cpp, cpp-vector-add-host-setup, eval=FALSE, echo=TRUE, size='small', code.cap="Step 1 of the CUDA application lifecycle: Allocating and initializing vectors in host (CPU) memory."}
// --- 1. Host-side Setup ---
int N = 1024 * 1024;
size_t size = N * sizeof(float);

// Allocate host memory for vectors A, B, and C
float *h_A = (float *)malloc(size);
float *h_B = (float *)malloc(size);
float *h_C = (float *)malloc(size);

// Initialize host vectors
for (int i = 0; i < N; ++i)
{
  h_A[i] = 1.0f;
  h_B[i] = 2.0f;
}
```

#### Device Memory Allocation and Data Transfer

Next, we allocate the corresponding memory buffers on the Device using `cudaMalloc`. We use the prefix `d_` to denote these device pointers. Once the device memory is allocated, we copy the input data from the host vectors to their device counterparts using `cudaMemcpy` with the `cudaMemcpyHostToDevice` kind.

```{cpp, cpp-vector-add-device-setup, eval=FALSE, echo=TRUE, size='small', code.cap="Steps 2 and 3: Allocating memory on the device (GPU) with cudaMalloc and copying data from host to device with cudaMemcpy."}
// --- 2. Device-side Memory Allocation ---
// Declare device pointers
float *d_A, *d_B, *d_C;
// Allocate memory on the GPU for each vector
cudaMalloc(&d_A, size);
cudaMalloc(&d_B, size);
cudaMalloc(&d_C, size);

// --- 3. Copy Input Data from Host to Device ---
printf("Copying input data from Host to Device...\n");
cudaMemcpy(d_A, h_A, size, cudaMemcpyHostToDevice);
cudaMemcpy(d_B, h_B, size, cudaMemcpyHostToDevice);
```

#### Kernel Execution

With the data now resident on the GPU, we can launch our `vectorAdd` kernel. We calculate the necessary grid and block dimensions and then call the kernel, passing it the device pointers. The `cudaDeviceSynchronize()` call is crucial, as it forces the host to wait until the device has completed its computation.

```{cpp, cpp-vector-add-kernel-execution, eval=FALSE, echo=TRUE, size='small', code.cap="Step 4: Launching the vectorAdd kernel on the device and using cudaDeviceSynchronize to wait for its completion."}
// --- 4. Launch the Kernel ---
int THREADS_PER_BLOCK = 256;
int BLOCKS_PER_GRID = (N + THREADS_PER_BLOCK - 1) / THREADS_PER_BLOCK;

printf("Launching vectorAdd kernel...\n");
vectorAdd<<<BLOCKS_PER_GRID, THREADS_PER_BLOCK>>>(d_A, d_B, d_C, N);

// Block host execution until the device kernel finishes
cudaDeviceSynchronize();
printf("Kernel execution finished.\n");
```

#### Retrieving Results

After the kernel finishes, the result vector `d_C` exists in the GPU's memory. To use it on the CPU, we must copy it back to our host vector `h_C` using `cudaMemcpy` with the `cudaMemcpyDeviceToHost` kind.

```{cpp, cpp-vector-add-copy-back, eval=FALSE, echo=TRUE, size='small', code.cap="Step 5: Copying the computational results from device memory back to host memory."}
// --- 5. Copy Result Data from Device to Host ---
printf("Copying result data from Device to Host...\n");
cudaMemcpy(h_C, d_C, size, cudaMemcpyDeviceToHost);
```

#### Verification and Cleanup

Finally, back on the host, we can verify the results to ensure the computation was correct. It is imperative to then free all allocated memory on both the Device (with `cudaFree`) and the Host (with `free`) to prevent memory leaks.

```{cpp, cpp-vector-add-verify-cleanup, eval=FALSE, echo=TRUE, size='small', code.cap="Steps 6 and 7: Verifying the results on the host and freeing all allocated host and device memory."}
// --- 6. Verification (on the host) ---
bool success = true;
// Check all elements for correctness
for (int i = 0; i < N; ++i)
{
  if (h_C[i] != 3.0f)
  {
    printf("Error at index %d: Expected 3.0, got %f\n", i, h_C[i]);
    success = false;
    break;
  }
}
if (success)
{
  // Print one of the results to show it worked
  printf("Verification successful! e.g., h_C[100] = %f\n", h_C[100]);
}

// --- 7. Cleanup ---
// Free device memory
cudaFree(d_A);
cudaFree(d_B);
cudaFree(d_C);
// Free host memory
free(h_A);
free(h_B);
free(h_C);
```

This complete, functional program demonstrates that the separate memory space of the Host and Device necessitate an explicit data pipeline, controlled by the programmer. This workflow is the fundamental pattern underpinning the vast majority of CUDA applications and is the final prerequisite for building our MCMC sampler.

## Parallel Randomness and the CUDA Library Ecosystem

The `vectorAdd` example was entirely deterministic. Given the same inputs, it will always produce the same output. However, many fundamental methods in statistics are stochastic. Simulation-based techniques such as the Bootstrap, permutation tests, and our chosen case study, MCMC, are central to modern statistical practice. To implement these on a GPU, we require a robust and statistically sound source of random numbers that is safe to use in a massively parallel environment.

This requirement reveals a critical pitfall and an important opportunity. A programmer accustomed to a CPU-only environment might be tempted to use a standard C function like `rand()` inside a CUDA kernel. This approach is fundamentally flawed and leads to incorrect results. Standard random number generators (RNG) typically rely on a single, hidden global state that is updated each time a number is generated. If thousands of GPU threads call a function simultaneously, they create a "race condition", all attempting to read and update this single state at once. This not only destroys the statistical properties of the resulting number sequences but also makes the simulation entirely non-reproducible.

The solution is to use libraries specifically designed for the parallel architecture of the GPU. The CUDA toolkit includes a rich ecosystem of high-performance libraries that provide optimized implementations of common computational primitives. These libraries, such as cuBLAS (for linear algebra), cuFFT (for Fourier Transforms), and cuDNN (for deep neural networks), are the foundational building blocks upon which many industry-standard, GPU-accelerated tools, including PyTorch, TensorFlow, and even XGBoost, are built. For our purposes, the most relevant of these is the `cuRAND` library.

`cuRAND` is designed to solve the parallel RNG problem by adhering to a simpler but powerful principle: instead of a single global generator, each thread is given its own independent RNG state. This ensures that the streams of random numbers produced by each thread are statistically independent and that there is no interference or contention between threads. The key components for using `cuRAND` are:

* `curandState_t`: A struct that holds the state of a single RNG. In practice, we allocate an array of these states on the GPU, one for each thread.
* `curand_init()`: A device-side function used within a kernel to initialize the state for each thread. It is crucial to seed each thread's generator uniquely to ensure the random number streams don't overlap. A combination of a user-provided seed and the thread's global index is a standard approach.
* `curand_uniform()` / `curand_normal()`: Device-side functions that take a pointer to a thread's state, generate a random number from the specified distribution, and update the state for the next call.

The following example demonstrates how to use `cuRAND` to generate an array of `N` random numbers, where each number is generated by a separate, independent thread. The process requires two distinct kernels: one to set up the generator states and a second to use them.

### Initializing RNG States with a Setup Kernel

The first kernel is responsible for initializing a unique `curandState_t` for each thread in the grid.

```{cpp, cpp-curand-setup-kernel, eval=FALSE, echo=TRUE, size='small', code.cap="The cuRAND setup kernel, where each thread initializes its own independent random number generator state."}
/*
 * Kernel 1: Initializes the cuRAND state for each thread.
 * Each thread gets a unique state based on its ID and a seed.
 */
__global__ void setup_kernel(curandState_t *states, unsigned long seed)
{
  int idx = blockIdx.x * blockDim.x + threadIdx.x;
  // Initialize the generator state for this thread
  curand_init(seed,          // The seed for the generator
              idx,           // A unique sequence number for each thread
              0,             // A 0 offset
              &states[idx]); // The address of the state to initialize
}

```

### Generating Numbers with a Parallel Kernel

This second kernel uses the initialized states to generate the random numbers. For efficiency, each thread copies its state from slow global memory to a fast register (`localState`), performs the generation, and then copies the updated state back.

```{cpp, cpp-curand-generate-kernel, eval=FALSE, echo=TRUE, size='small', code.cap="The cuRAND generation kernel, where each thread uses its unique state to generate a parallel, statistically independent random number."}
/*
 * Kernel 2: Uses the initialized states to generate random numbers.
 */
__global__ void generate_kernel(float *output, curandState_t *states,
                                int N) {
    int idx = blockIdx.x * blockDim.x + threadIdx.x;

    if (idx < N) {
        // Copy the state from global memory to a register for this thread
        curandState_t localState = states[idx];
        
        // Generate a random float between 0.0 and 1.0
        output[idx] = curand_uniform(&localState);

        // Copy the updated state back to global memory
        states[idx] = localState;
    }
}

```

### Orchestrating the cuRAND Workflow

The host code manages the allocation of memory for both the output array and the `cuRAND` states, and then launches the two kernels in sequence.

```{cpp, cpp-curand-host-orchestration, eval=FALSE, echo=TRUE, size='small', code.cap="Host-side code that orchestrates the cuRAND example, including memory management and the sequential launch of the setup and generation kernels."}
int N = 1024;
size_t size = N * sizeof(float);

// --- 1. Host-side Setup ---
float *h_output = (float *)malloc(size);

// --- 2. Device-side Memory Allocation ---
float *d_output;
curandState_t *d_states;
cudaMalloc(&d_output, size);
cudaMalloc(&d_states, N * sizeof(curandState_t));

// --- 3. Kernel Configuration ---
int THREADS_PER_BLOCK = 256;
int BLOCKS_PER_GRID = (N + THREADS_PER_BLOCK - 1) / THREADS_PER_BLOCK;

// --- 4. Launch Setup Kernel ---
// Use the current time as a seed for the random number generator
setup_kernel<<<BLOCKS_PER_GRID, THREADS_PER_BLOCK>>>(d_states, time(NULL));

// --- 5. Launch Generation Kernel ---
generate_kernel<<<BLOCKS_PER_GRID, 
                  THREADS_PER_BLOCK>>>(d_output, d_states, N);
cudaDeviceSynchronize();

// --- 6. Copy results back to host ---
cudaMemcpy(h_output, d_output, size, cudaMemcpyDeviceToHost);

// --- 7. Verification ---
printf("Generated random numbers (first 10):\n");
for (int i = 0; i < 10; ++i)
{
  printf("h_output[%d] = %f\n", i, h_output[i]);
}

// --- 8. Cleanup ---
cudaFree(d_states);
cudaFree(d_output);
free(h_output);

return 0;
```

This example illustrates the necessity of using specialized, hardware-aware libraries like `cuRAND` to perform even fundamental statistical operations in a parallel environment. The ability to generate statistically valid random numbers for each thread independently is the final conceptual tool required before we can assemble our complete MCMC sampler.

## Assembling a Massively Parallel MCMC Sampler

With the foundational concepts of the CUDA programming model established, we are now ready to assemble them to solve our statistical problem. This section deconstructs a complete, end-to-end MCMC sampler that leverages the GPU to run thousands of independent Markov chains in parallel. This approach transforms a traditionally time-consuming, sequential task into a high-throughput computation.

The final program consists of three key pieces of device code: a helper function to compute the log-posterior, a kernel to initialize the random number generators, and the main MCMC kernel. Orchestration occurs within the Host-side `main` function.

### MCMC Device Code: Kernels and Helpers

The core of the application resides in the code executed on the GPU. This is where the statistical computation happens. The Device-side logic is in the kernels below, in addition to the `cuRAND` setup kernel presented in the previous section.

#### The `log_posterior` Device Function

To keep our main kernel clean and modular, we first define a `__device__` function. Unlike a `__global__` kernel, a `__device__` function cannot be called from the host, but only by other kernels or function on the device. It serves as a reusable helper function. This one calculates the log-posterior for a given value of $\mu$ by summing the log-prior and the log-likelihood

```{cpp, cpp-mcmc-log-posterior, eval=FALSE, echo=TRUE, size='small', code.cap="A __device__ helper function to compute the log-posterior, callable only from the GPU."}
// A __device__ function can only be called from the device (e.g., from a 
// kernel).
__device__ float log_posterior(float mu, const float *d_data, int N,
                               float sigma2, float mu0, float tau2_0)
{
  // 1. Calculate log prior: log( N(mu | mu0, tau2_0) )
  float log_prior = -0.5f * powf(mu - mu0, 2) / tau2_0;

  // 2. Calculate log likelihood: log( N(data | mu, sigma2) )
  float log_likelihood = 0.0f;
  for (int i = 0; i < N; ++i)
  {
    log_likelihood += -0.5f * powf(d_data[i] - mu, 2) / sigma2;
  }

  return log_prior + log_likelihood;
}
```

#### The Main `mcmc_kernel`

This is the workhorse of our application. A single launch of this kernel executes the *entire* MCMC simulation. Each thread is assigned to a single chain and runs a `for` loop for the required number of iterations. This design is highly efficient as it minimizes the overhead of launching many small kernels from the host.

Inside the kernel, each thread:

1. Identifies its unique chain using the `global_idx` pattern.
2. Initializes its own `curandState` and starting parameter `current_mu`.
3. Enters the main MCMC loop. In each iteration, it proposes a new `mu`, computes the log-posteriors for the current and proposed values by calling our `log_posterior` helper, and performs the Metropolis-Hastings acceptance check.
4. After all iterations are complete, it stores the final sample of its chain in the global output array.

```{cpp, cpp-mcmc-main-kernel, eval=FALSE, echo=TRUE, size='small', code.cap="The main MCMC kernel where each thread executes a complete Markov chain in parallel."}
// --- The MCMC Kernel ---
__global__ void mcmc_kernel(float *d_output, const float *d_data,
                            curandState_t *states, int N_data, int N_chains,
                            int N_iters, int N_burn_in, float sigma2,
                            float mu0, float tau2_0, float prop_sigma)
{

  int idx = blockIdx.x * blockDim.x + threadIdx.x;

  if (idx < N_chains)
  {
    curandState_t local_rand_state = states[idx];
    float current_mu = mu0; // Start each chain at the prior mean

    // Main MCMC loop
    for (int i = 0; i < N_iters + N_burn_in; ++i)
    {
      // Propose a new mu using the Normal distribution from cuRAND
      float proposed_mu = current_mu +
        curand_normal(&local_rand_state) * prop_sigma;

      // Calculate log posterior for current and proposed mu
      float log_post_current = log_posterior(current_mu, d_data, N_data,
                                             sigma2, mu0, tau2_0);
      float log_post_proposed = log_posterior(proposed_mu, d_data, N_data,
                                              sigma2, mu0, tau2_0);

      // Acceptance check in log-space
      float log_alpha = fminf(0.0f, log_post_proposed - log_post_current);
      if (logf(curand_uniform(&local_rand_state)) < log_alpha)
      {
        current_mu = proposed_mu;
      }
    }

    // After burn-in and iterations, store the final sample of the chain
    d_output[idx] = current_mu;
    states[idx] = local_rand_state; // Save the updated random state
  }
}
```

### Host Code: Orchestrating the Sampler

The host is responsible for setting up the problem, managing the GPU, and analysing the final results.

#### Problem Setup and Host Data Management

First, we define all the parameters for our simulation and generate synthetic data on the host. This data will serve as the "observed" data for our model.

```{cpp, cpp-mcmc-host-setup, eval=FALSE, echo=TRUE, size='small', code.cap="Host-side setup for the MCMC simulation, including defining parameters and generating synthetic data."}
// --- 1. Problem Setup ---
int N_data = 1000;        // Number of data points
int N_chains = 1024 * 16; // Number of parallel MCMC chains to run (16,384)
int N_iters = 2000;       // MCMC iterations per chain
int N_burn_in = 500;      // Burn-in iterations to discard

// True parameters for data generation
float true_mu = 10.0f;
float sigma2 = 4.0f;

// Priors for mu ~ N(mu0, tau2_0)
float mu0 = 0.0f;
float tau2_0 = 100.0f;

// MCMC proposal width
float prop_sigma = 1.0f;

// --- 2. Host-side Data Generation and Memory Allocation ---
float *h_data = (float *)malloc(N_data * sizeof(float));
float *h_output = (float *)malloc(N_chains * sizeof(float));

// Generate synthetic data from the true model
srand(time(NULL));
for (int i = 0; i < N_data; ++i)
{
  // A simple way to get a standard normal-like random number
  float u1 = rand() / (float)RAND_MAX;
  float u2 = rand() / (float)RAND_MAX;
  float rand_std_normal = sqrtf(-2.0f * logf(u1)) * cosf(2.0f * M_PI * u2);
  h_data[i] = true_mu + sqrtf(sigma2) * rand_std_normal;
}
```

#### Device Memory Management and Kernel Launch

Next, we follow the data pipeline pattern: 1. allocate memory on the device with `cudaMalloc`, 2. copy the host data to the device with `cudaMemcpy`, and 3. set up the `cuRAND` states by launching the `setup_kernel`. Finally, we launch our main `mcmc_kernel`. Note that we only launch it **once**.

```{cpp, cpp-mcmc-device-launch, eval=FALSE, echo=TRUE, size='small', code.cap="Core host orchestration: allocating device memory, setting up cuRAND states, and launching the MCMC kernel."}
// --- 3. Device-side Memory Allocation ---
float *d_data, *d_output;
curandState_t *d_states;
cudaMalloc(&d_data, N_data * sizeof(float));
cudaMalloc(&d_output, N_chains * sizeof(float));
cudaMalloc(&d_states, N_chains * sizeof(curandState_t));

// --- 4. Copy data and Setup Random States ---
cudaMemcpy(d_data, h_data, N_data * sizeof(float), cudaMemcpyHostToDevice);

int THREADS_PER_BLOCK = 256;
int BLOCKS_PER_GRID = (N_chains + THREADS_PER_BLOCK - 1) / THREADS_PER_BLOCK;

setup_kernel<<<BLOCKS_PER_GRID, THREADS_PER_BLOCK>>>(d_states, time(NULL));

// --- 5. Launch the MCMC Kernel ---
printf("Launching %d parallel MCMC chains...\n", N_chains);
mcmc_kernel<<<BLOCKS_PER_GRID, THREADS_PER_BLOCK>>>(
    d_output, d_data, d_states, N_data, N_chains, N_iters, N_burn_in,
    sigma2, mu0, tau2_0, prop_sigma);
cudaDeviceSynchronize();
printf("MCMC simulation finished.\n");
```

#### Retrieving Results and Cleanup

After the simulation is complete, we copy the final samples from each chain back to the host and perform a simple analysis by calculating the posterior mean. Finally, we free all allocated memory on both the device and the host.

```{cpp, cpp-mcmc-host-cleanup, eval=FALSE, echo=TRUE, size='small', code.cap="Final steps on the host: retrieving results from the GPU, performing analysis, and freeing all memory."}
// --- 6. Copy results back and analyze ---
cudaMemcpy(h_output, d_output, N_chains * sizeof(float),
           cudaMemcpyDeviceToHost);

float posterior_mean = 0.0f;
for (int i = 0; i < N_chains; ++i)
{
  posterior_mean += h_output[i];
}
posterior_mean /= N_chains;

printf("\n--- Analysis ---\n");
printf("Posterior Mean of mu (from %d samples): %f\n", N_chains,
       posterior_mean);
printf("True Mean of mu was: %f\n", true_mu);

// --- 7. Cleanup ---
cudaFree(d_data);
cudaFree(d_output);
cudaFree(d_states);
free(h_data);
free(h_output);
```

This complete example demonstrates how the various components of the CUDA programming model are integrated to solve a non-trivial statistical problem. By reframing the task from running one long chain to running thousands of shorter ones, we can leverage the massive parallelism of the GPU to dramatically reduce the time required for Bayesian inference.
