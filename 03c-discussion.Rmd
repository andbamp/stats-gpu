## Discussion: Empirical Findings

The empirical benchmarks conducted on Falkon and XGBoost provide compelling, practical evidence to support the central arguments of this thesis. Across both case studies, a clear performance ladder emerged, with traditional, exact algorithms proving to be computationally non-competitive. Their failure to scale empirically confirms their status as a "computational wall" for modern, large-scale datasets. This highlights a critical insight: algorithmic innovation is a prerequisite for speed. The most profound performance leap in XGBoost, for instance, was not from the GPU itself, but from the adoption of the `hist` algorithm. This algorithmic transformation was the key enabler, as it reshaped the computational profile from a CPU-bound sorting problem to a memory-bandwidth-bound aggregation problem, making it perfectly suited for the GPU's parallel architecture.

This synergy between an optimized algorithm and parallel hardware is what unlocks state-of-the-art performance. While CPU methods can be faster for smaller tasks before the "inflection point" where GPU overhead is overcome, the GPU implementations consistently deliver a dramatic, additional layer of acceleration on large datasets. This speedup fundamentally enhances the statistical rigor of the entire modelling process. The ability to train a model in minutes instead of hours makes computationally expensive but more robust techniques, such as k-fold cross-validation, practically feasible. It allows for a more exhaustive exploration of the hyperparameter space, leading to better-tuned models. This newfound agility facilitates a more iterative and scientific approach to modelling, where multiple feature engineering strategies and competing architectures can be rapidly tested.

Crucially, the experiments confirm that these immense performance gains do not come at the cost of model integrity. The accuracy metrics for both Falkon and XGBoost remained stable and comparable between the fast GPU versions and their slower CPU counterparts. Furthermore, the diagnostic analysis of the final XGBoost model underscores that the end result is not an opaque "black box", but a trustworthy and fully interpretable statistical model. The experiments also revealed important practical trade-offs, such as the point of diminishing returns for approximation quality in Falkon, where increasing the number of Nystr√∂m centres eventually plateaued due to the numerical precision limits of the hardware.

Ultimately, the empirical findings of this chapter are clear. GPU acceleration does more than just make existing workflows faster, but rather expands the scope of what is statistically sound and practically achievable. While the public datasets used here are essential for reproducible science, the true value of this acceleration is most profound for the unique, high-stakes datasets found in scientific and industrial research. By empowering practitioners to build more robust, better-validated, and more deeply understood models, this technology enables more ambitious research and more reliable data-driven discovery.
