{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "gpuType": "A100",
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Introduction to CUDA C++"
      ],
      "metadata": {
        "id": "tE1Ab4mdQ7vx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t20BjxCJdw1e",
        "outputId": "daaee6f3-9a38-4c6f-ba2f-04dcf7105b7e"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fri Jul 25 19:10:29 2025       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  NVIDIA A100-SXM4-40GB          Off |   00000000:00:04.0 Off |                    0 |\n",
            "| N/A   33C    P0             47W /  400W |       0MiB /  40960MiB |      0%      Default |\n",
            "|                                         |                        |             Disabled |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "|  No running processes found                                                             |\n",
            "+-----------------------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Example 1"
      ],
      "metadata": {
        "id": "BuynrDiPVnOY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile hello_gpu.cu\n",
        "// File: hello_gpu.cu\n",
        "#include <stdio.h>\n",
        "\n",
        "/*\n",
        " * Kernel: A function that runs on the device (GPU).\n",
        " * The __global__ specifier marks it as such.\n",
        " * This kernel will be executed by many threads in parallel.\n",
        " */\n",
        "__global__ void hello_from_gpu()\n",
        "{\n",
        "    printf(\"Hello, World! from the GPU!\\n\");\n",
        "}\n",
        "\n",
        "/*\n",
        " * Host: The main function that runs on the CPU.\n",
        " * It orchestrates the program and launches kernels on the device.\n",
        " */\n",
        "int main()\n",
        "{\n",
        "    // 1. A message from the host CPU.\n",
        "    printf(\"Hello from the host CPU before launching the kernel.\\n\");\n",
        "\n",
        "    // 2. The Host launches the kernel on the Device.\n",
        "    // We launch a \"grid\" of 1 block, containing 4 threads.\n",
        "    hello_from_gpu<<<1, 4>>>();\n",
        "\n",
        "    // 3. The host must wait for the device to finish its work before exiting.\n",
        "    // cudaDeviceSynchronize() is a barrier that pauses the host\n",
        "    // until all previously launched device tasks are complete.\n",
        "    cudaDeviceSynchronize();\n",
        "\n",
        "    // 4. A final message from the host CPU.\n",
        "    printf(\"Kernel launch finished. Hello from the host CPU again.\\n\");\n",
        "\n",
        "    return 0;\n",
        "}\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dZKElLOiezLe",
        "outputId": "59ded20b-8700-49e8-9ed1-03c086dbc219"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing hello_gpu.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc hello_gpu.cu -o hello_gpu \\\n",
        "    -gencode arch=compute_75,code=sm_75 \\\n",
        "    -gencode arch=compute_80,code=sm_80 \\\n",
        "    -gencode arch=compute_80,code=compute_80"
      ],
      "metadata": {
        "id": "vq_VN9qgfRRC"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!./hello_gpu"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CfQf-O1wfgRP",
        "outputId": "0fba9f1f-8dad-4f48-ce0f-7ff71b94ae6e"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello from the host CPU before launching the kernel.\n",
            "Hello, World! from the GPU!\n",
            "Hello, World! from the GPU!\n",
            "Hello, World! from the GPU!\n",
            "Hello, World! from the GPU!\n",
            "Kernel launch finished. Hello from the host CPU again.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Example 2"
      ],
      "metadata": {
        "id": "rGLxc5A4VU9T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile demo_indices.cu\n",
        "// File: demo_indices.cu\n",
        "#include <stdio.h>\n",
        "#include <cuda_runtime.h>\n",
        "\n",
        "/*\n",
        " * Kernel to demonstrate thread indexing.\n",
        " * Each thread calculates its global index, and the first thread\n",
        " * in each block prints its identity to show the hierarchy.\n",
        " */\n",
        "__global__ void demoIndices(int N)\n",
        "{\n",
        "\n",
        "    // 1. Calculate the unique global index for this thread.\n",
        "    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "\n",
        "    // 2. Bounds check to ensure we don't do work for padded threads.\n",
        "    if (idx < N)\n",
        "    {\n",
        "        // 3. To keep output clean, we will only have the FIRST thread\n",
        "        // of each block print its information. This confirms that\n",
        "        // multiple blocks are being launched and are aware of their IDs.\n",
        "        if (threadIdx.x == 0)\n",
        "        {\n",
        "            printf(\"GPU: Block ID = %d, Thread ID = %d -> Calculated Global Index = %d\\n\",\n",
        "                   blockIdx.x, threadIdx.x, idx);\n",
        "        }\n",
        "    }\n",
        "}\n",
        "\n",
        "/*\n",
        " * Host: The main function that runs on the CPU.\n",
        " */\n",
        "int main()\n",
        "{\n",
        "    // Define the size of our conceptual data.\n",
        "    // We'll use a small N that is not a perfect multiple of THREADS_PER_BLOCK\n",
        "    // to show that the bounds check and grid calculation work correctly.\n",
        "    int N = 500;\n",
        "\n",
        "    // Define the number of threads per block. 256 is a common, efficient choice.\n",
        "    int THREADS_PER_BLOCK = 256;\n",
        "\n",
        "    // Calculate the number of blocks needed in the grid, using the\n",
        "    // integer arithmetic trick for ceiling division.\n",
        "    // For N=500 and Threads=256, this will be ceil(500/256) = 2 blocks.\n",
        "    int BLOCKS_PER_GRID = (N + THREADS_PER_BLOCK - 1) / THREADS_PER_BLOCK;\n",
        "\n",
        "    printf(\"Host: Problem size N = %d\\n\", N);\n",
        "    printf(\"Host: Threads per Block = %d\\n\", THREADS_PER_BLOCK);\n",
        "    printf(\"Host: Calculated Blocks in Grid = %d\\n\\n\", BLOCKS_PER_GRID);\n",
        "\n",
        "    // Launch the kernel with our calculated grid and block dimensions.\n",
        "    demoIndices<<<BLOCKS_PER_GRID, THREADS_PER_BLOCK>>>(N);\n",
        "\n",
        "    // Synchronize to make sure the kernel finishes and we see its output.\n",
        "    cudaDeviceSynchronize();\n",
        "\n",
        "    printf(\"\\nHost: Kernel finished.\\n\");\n",
        "\n",
        "    return 0;\n",
        "}\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BFpnDrqiffWH",
        "outputId": "303fd821-395e-47e7-9801-64ddad45be8f"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing demo_indices.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc demo_indices.cu -o demo_indices \\\n",
        "    -gencode arch=compute_75,code=sm_75 \\\n",
        "    -gencode arch=compute_80,code=sm_80 \\\n",
        "    -gencode arch=compute_80,code=compute_80"
      ],
      "metadata": {
        "id": "S2aNl6wgf8WQ"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!./demo_indices"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YDyOLoVgf-G8",
        "outputId": "6c0345b0-b07e-46ca-8be5-400d89d245f9"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Host: Problem size N = 500\n",
            "Host: Threads per Block = 256\n",
            "Host: Calculated Blocks in Grid = 2\n",
            "\n",
            "GPU: Block ID = 0, Thread ID = 0 -> Calculated Global Index = 0\n",
            "GPU: Block ID = 1, Thread ID = 0 -> Calculated Global Index = 256\n",
            "\n",
            "Host: Kernel finished.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Example 3"
      ],
      "metadata": {
        "id": "HSk0L2qagAMw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile vector_add.cu\n",
        "// File: vector_add.cu\n",
        "#include <stdio.h>\n",
        "#include <stdlib.h>  // For malloc/free\n",
        "#include <stdbool.h> // For bool type\n",
        "\n",
        "// The vectorAdd kernel from the previous section.\n",
        "__global__ void vectorAdd(const float *d_A, const float *d_B, float *d_C, int N)\n",
        "{\n",
        "    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "    if (idx < N)\n",
        "    {\n",
        "        d_C[idx] = d_A[idx] + d_B[idx];\n",
        "    }\n",
        "}\n",
        "\n",
        "/*\n",
        " * Host: The main function that runs on the CPU.\n",
        " */\n",
        "int main()\n",
        "{\n",
        "    // --- 1. Host-side Setup ---\n",
        "    int N = 1024 * 1024;\n",
        "    size_t size = N * sizeof(float);\n",
        "\n",
        "    // Allocate host memory for vectors A, B, and C\n",
        "    float *h_A = (float *)malloc(size);\n",
        "    float *h_B = (float *)malloc(size);\n",
        "    float *h_C = (float *)malloc(size);\n",
        "\n",
        "    // Initialize host vectors\n",
        "    for (int i = 0; i < N; ++i)\n",
        "    {\n",
        "        h_A[i] = 1.0f;\n",
        "        h_B[i] = 2.0f;\n",
        "    }\n",
        "\n",
        "    // --- 2. Device-side Memory Allocation ---\n",
        "    // Declare device pointers\n",
        "    float *d_A, *d_B, *d_C;\n",
        "    // Allocate memory on the GPU for each vector\n",
        "    cudaMalloc(&d_A, size);\n",
        "    cudaMalloc(&d_B, size);\n",
        "    cudaMalloc(&d_C, size);\n",
        "\n",
        "    // --- 3. Copy Input Data from Host to Device ---\n",
        "    printf(\"Copying input data from Host to Device...\\n\");\n",
        "    cudaMemcpy(d_A, h_A, size, cudaMemcpyHostToDevice);\n",
        "    cudaMemcpy(d_B, h_B, size, cudaMemcpyHostToDevice);\n",
        "\n",
        "    // --- 4. Launch the Kernel ---\n",
        "    int THREADS_PER_BLOCK = 256;\n",
        "    int BLOCKS_PER_GRID = (N + THREADS_PER_BLOCK - 1) / THREADS_PER_BLOCK;\n",
        "\n",
        "    printf(\"Launching vectorAdd kernel...\\n\");\n",
        "    vectorAdd<<<BLOCKS_PER_GRID, THREADS_PER_BLOCK>>>(d_A, d_B, d_C, N);\n",
        "\n",
        "    // Block host execution until the device kernel finishes\n",
        "    cudaDeviceSynchronize();\n",
        "    printf(\"Kernel execution finished.\\n\");\n",
        "\n",
        "    // --- 5. Copy Result Data from Device to Host ---\n",
        "    printf(\"Copying result data from Device to Host...\\n\");\n",
        "    cudaMemcpy(h_C, d_C, size, cudaMemcpyDeviceToHost);\n",
        "\n",
        "    // --- 6. Verification (on the host) ---\n",
        "    bool success = true;\n",
        "    // Check all elements for correctness\n",
        "    for (int i = 0; i < N; ++i)\n",
        "    {\n",
        "        if (h_C[i] != 3.0f)\n",
        "        {\n",
        "            printf(\"Error at index %d: Expected 3.0, got %f\\n\", i, h_C[i]);\n",
        "            success = false;\n",
        "            break;\n",
        "        }\n",
        "    }\n",
        "    if (success)\n",
        "    {\n",
        "        // Print one of the results to show it worked\n",
        "        printf(\"Verification successful! e.g., h_C[100] = %f\\n\", h_C[100]);\n",
        "    }\n",
        "\n",
        "    // --- 7. Cleanup ---\n",
        "    // Free device memory\n",
        "    cudaFree(d_A);\n",
        "    cudaFree(d_B);\n",
        "    cudaFree(d_C);\n",
        "    // Free host memory\n",
        "    free(h_A);\n",
        "    free(h_B);\n",
        "    free(h_C);\n",
        "\n",
        "    return 0;\n",
        "}\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yGrwMRq0gftM",
        "outputId": "d38cea66-bdea-478e-a2b2-ea1a1cfdac7f"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing vector_add.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc vector_add.cu -o vector_add \\\n",
        "    -gencode arch=compute_75,code=sm_75 \\\n",
        "    -gencode arch=compute_80,code=sm_80 \\\n",
        "    -gencode arch=compute_80,code=compute_80"
      ],
      "metadata": {
        "id": "GtGZzIAtgksX"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!./vector_add"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sZbrZ7ZKgmO9",
        "outputId": "fd6aca15-9dd5-475a-d3f4-418cfa422471"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Copying input data from Host to Device...\n",
            "Launching vectorAdd kernel...\n",
            "Kernel execution finished.\n",
            "Copying result data from Device to Host...\n",
            "Verification successful! e.g., h_C[100] = 3.000000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Example 4"
      ],
      "metadata": {
        "id": "Nfypfq4OgnS_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile random_generation.cu\n",
        "// File: random_generation.cu\n",
        "#include <stdio.h>\n",
        "#include <stdlib.h>\n",
        "#include <time.h>          // For time()\n",
        "#include <curand_kernel.h> // Required for cuRAND device functions\n",
        "\n",
        "/*\n",
        " * Kernel 1: Initializes the cuRAND state for each thread.\n",
        " * Each thread gets a unique state based on its ID and a seed.\n",
        " */\n",
        "__global__ void setup_kernel(curandState_t *states, unsigned long seed)\n",
        "{\n",
        "    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "    // Initialize the generator state for this thread\n",
        "    curand_init(seed,          // The seed for the generator\n",
        "                idx,           // A unique sequence number for each thread\n",
        "                0,             // A 0 offset\n",
        "                &states[idx]); // The address of the state to initialize\n",
        "}\n",
        "\n",
        "/*\n",
        " * Kernel 2: Uses the initialized states to generate random numbers.\n",
        " */\n",
        "__global__ void generate_kernel(float *output, curandState_t *states, int N)\n",
        "{\n",
        "    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "\n",
        "    if (idx < N)\n",
        "    {\n",
        "        // Copy the state from global memory to a register for this thread\n",
        "        curandState_t localState = states[idx];\n",
        "\n",
        "        // Generate a random float between 0.0 and 1.0\n",
        "        output[idx] = curand_uniform(&localState);\n",
        "\n",
        "        // Copy the updated state back to global memory\n",
        "        states[idx] = localState;\n",
        "    }\n",
        "}\n",
        "\n",
        "// The main host function\n",
        "int main()\n",
        "{\n",
        "    int N = 1024;\n",
        "    size_t size = N * sizeof(float);\n",
        "\n",
        "    // --- 1. Host-side Setup ---\n",
        "    float *h_output = (float *)malloc(size);\n",
        "\n",
        "    // --- 2. Device-side Memory Allocation ---\n",
        "    float *d_output;\n",
        "    curandState_t *d_states;\n",
        "    cudaMalloc(&d_output, size);\n",
        "    cudaMalloc(&d_states, N * sizeof(curandState_t));\n",
        "\n",
        "    // --- 3. Kernel Configuration ---\n",
        "    int THREADS_PER_BLOCK = 256;\n",
        "    int BLOCKS_PER_GRID = (N + THREADS_PER_BLOCK - 1) / THREADS_PER_BLOCK;\n",
        "\n",
        "    // --- 4. Launch Setup Kernel ---\n",
        "    // Use the current time as a seed for the random number generator\n",
        "    setup_kernel<<<BLOCKS_PER_GRID, THREADS_PER_BLOCK>>>(d_states, time(NULL));\n",
        "\n",
        "    // --- 5. Launch Generation Kernel ---\n",
        "    generate_kernel<<<BLOCKS_PER_GRID, THREADS_PER_BLOCK>>>(d_output, d_states, N);\n",
        "    cudaDeviceSynchronize();\n",
        "\n",
        "    // --- 6. Copy results back to host ---\n",
        "    cudaMemcpy(h_output, d_output, size, cudaMemcpyDeviceToHost);\n",
        "\n",
        "    // --- 7. Verification ---\n",
        "    printf(\"Generated random numbers (first 10):\\n\");\n",
        "    for (int i = 0; i < 10; ++i)\n",
        "    {\n",
        "        printf(\"h_output[%d] = %f\\n\", i, h_output[i]);\n",
        "    }\n",
        "\n",
        "    // --- 8. Cleanup ---\n",
        "    cudaFree(d_states);\n",
        "    cudaFree(d_output);\n",
        "    free(h_output);\n",
        "\n",
        "    return 0;\n",
        "}\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b_1iPXXvhEDa",
        "outputId": "8ece12ad-59a2-4b98-8cad-ae3328c00564"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing random_generation.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc random_generation.cu -o random_generation \\\n",
        "    -gencode arch=compute_75,code=sm_75 \\\n",
        "    -gencode arch=compute_80,code=sm_80 \\\n",
        "    -gencode arch=compute_80,code=compute_80 \\\n",
        "    -lcurand"
      ],
      "metadata": {
        "id": "I1sN6du5hEgo"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!./random_generation"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pirzlJ0GhGTm",
        "outputId": "2eb759cb-5540-46c4-a869-0d028dee2178"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated random numbers (first 10):\n",
            "h_output[0] = 0.195114\n",
            "h_output[1] = 0.337967\n",
            "h_output[2] = 0.433073\n",
            "h_output[3] = 0.350870\n",
            "h_output[4] = 0.618155\n",
            "h_output[5] = 0.775106\n",
            "h_output[6] = 0.623580\n",
            "h_output[7] = 0.296973\n",
            "h_output[8] = 0.197414\n",
            "h_output[9] = 0.939234\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Example 5"
      ],
      "metadata": {
        "id": "vHL6rVk4hH-P"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### CUDA C++ (GPU)"
      ],
      "metadata": {
        "id": "Xe_omFWHkkfb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile mcmc_sampler.cu\n",
        "// File: mcmc_sampler.cu\n",
        "#include <stdio.h>\n",
        "#include <stdlib.h>\n",
        "#include <time.h>\n",
        "#include <curand_kernel.h>\n",
        "#include <math.h> // For logf, expf, powf, sqrtf\n",
        "\n",
        "// --- Device helper function to compute the log posterior ---\n",
        "// A __device__ function can only be called from the device (e.g., from a kernel).\n",
        "__device__ float log_posterior(float mu, const float *d_data, int N, float sigma2, float mu0, float tau2_0)\n",
        "{\n",
        "    // 1. Calculate log prior: log( N(mu | mu0, tau2_0) )\n",
        "    float log_prior = -0.5f * powf(mu - mu0, 2) / tau2_0;\n",
        "\n",
        "    // 2. Calculate log likelihood: log( N(data | mu, sigma2) )\n",
        "    float log_likelihood = 0.0f;\n",
        "    for (int i = 0; i < N; ++i)\n",
        "    {\n",
        "        log_likelihood += -0.5f * powf(d_data[i] - mu, 2) / sigma2;\n",
        "    }\n",
        "\n",
        "    return log_prior + log_likelihood;\n",
        "}\n",
        "\n",
        "// --- The MCMC Kernel ---\n",
        "__global__ void mcmc_kernel(float *d_output, const float *d_data, curandState_t *states,\n",
        "                            int N_data, int N_chains, int N_iters, int N_burn_in,\n",
        "                            float sigma2, float mu0, float tau2_0, float prop_sigma)\n",
        "{\n",
        "\n",
        "    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "\n",
        "    if (idx < N_chains)\n",
        "    {\n",
        "        curandState_t local_rand_state = states[idx];\n",
        "        float current_mu = mu0; // Start each chain at the prior mean\n",
        "\n",
        "        // Main MCMC loop\n",
        "        for (int i = 0; i < N_iters + N_burn_in; ++i)\n",
        "        {\n",
        "            // Propose a new mu using the Normal distribution from cuRAND\n",
        "            float proposed_mu = current_mu + curand_normal(&local_rand_state) * prop_sigma;\n",
        "\n",
        "            // Calculate log posterior for current and proposed mu\n",
        "            float log_post_current = log_posterior(current_mu, d_data, N_data, sigma2, mu0, tau2_0);\n",
        "            float log_post_proposed = log_posterior(proposed_mu, d_data, N_data, sigma2, mu0, tau2_0);\n",
        "\n",
        "            // Acceptance check in log-space\n",
        "            float log_alpha = fminf(0.0f, log_post_proposed - log_post_current);\n",
        "            if (logf(curand_uniform(&local_rand_state)) < log_alpha)\n",
        "            {\n",
        "                current_mu = proposed_mu;\n",
        "            }\n",
        "        }\n",
        "\n",
        "        // After burn-in and iterations, store the final sample of the chain\n",
        "        d_output[idx] = current_mu;\n",
        "        states[idx] = local_rand_state; // Save the updated random state\n",
        "    }\n",
        "}\n",
        "\n",
        "// The setup_kernel from the previous section\n",
        "__global__ void setup_kernel(curandState_t *states, unsigned long seed)\n",
        "{\n",
        "    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "    curand_init(seed, idx, 0, &states[idx]);\n",
        "}\n",
        "\n",
        "// --- The Main Host Function ---\n",
        "int main()\n",
        "{\n",
        "    // --- 1. Problem Setup ---\n",
        "    int N_data = 1000;        // Number of data points\n",
        "    int N_chains = 1024 * 16; // Number of parallel MCMC chains to run (16,384)\n",
        "    int N_iters = 2000;       // MCMC iterations per chain\n",
        "    int N_burn_in = 500;      // Burn-in iterations to discard\n",
        "\n",
        "    // True parameters for data generation\n",
        "    float true_mu = 10.0f;\n",
        "    float sigma2 = 4.0f;\n",
        "\n",
        "    // Priors for mu ~ N(mu0, tau2_0)\n",
        "    float mu0 = 0.0f;\n",
        "    float tau2_0 = 100.0f;\n",
        "\n",
        "    // MCMC proposal width\n",
        "    float prop_sigma = 1.0f;\n",
        "\n",
        "    // --- 2. Host-side Data Generation and Memory Allocation ---\n",
        "    float *h_data = (float *)malloc(N_data * sizeof(float));\n",
        "    float *h_output = (float *)malloc(N_chains * sizeof(float));\n",
        "\n",
        "    // Generate synthetic data from the true model\n",
        "    // Note: This host-side RNG is simple for demonstration.\n",
        "    // In a real scenario, real data would be loaded.\n",
        "    srand(time(NULL));\n",
        "    for (int i = 0; i < N_data; ++i)\n",
        "    {\n",
        "        // A simple way to get a standard normal-like random number\n",
        "        float u1 = rand() / (float)RAND_MAX;\n",
        "        float u2 = rand() / (float)RAND_MAX;\n",
        "        float rand_std_normal = sqrtf(-2.0f * logf(u1)) * cosf(2.0f * M_PI * u2);\n",
        "        h_data[i] = true_mu + sqrtf(sigma2) * rand_std_normal;\n",
        "    }\n",
        "\n",
        "    // --- 3. Device-side Memory Allocation ---\n",
        "    float *d_data, *d_output;\n",
        "    curandState_t *d_states;\n",
        "    cudaMalloc(&d_data, N_data * sizeof(float));\n",
        "    cudaMalloc(&d_output, N_chains * sizeof(float));\n",
        "    cudaMalloc(&d_states, N_chains * sizeof(curandState_t));\n",
        "\n",
        "    // --- 4. Copy data and Setup Random States ---\n",
        "    cudaMemcpy(d_data, h_data, N_data * sizeof(float), cudaMemcpyHostToDevice);\n",
        "\n",
        "    int THREADS_PER_BLOCK = 256;\n",
        "    int BLOCKS_PER_GRID = (N_chains + THREADS_PER_BLOCK - 1) / THREADS_PER_BLOCK;\n",
        "\n",
        "    setup_kernel<<<BLOCKS_PER_GRID, THREADS_PER_BLOCK>>>(d_states, time(NULL));\n",
        "\n",
        "    // --- 5. Launch the MCMC Kernel ---\n",
        "    printf(\"Launching %d parallel MCMC chains...\\n\", N_chains);\n",
        "    mcmc_kernel<<<BLOCKS_PER_GRID, THREADS_PER_BLOCK>>>(\n",
        "        d_output, d_data, d_states, N_data, N_chains, N_iters, N_burn_in,\n",
        "        sigma2, mu0, tau2_0, prop_sigma);\n",
        "    cudaDeviceSynchronize();\n",
        "    printf(\"MCMC simulation finished.\\n\");\n",
        "\n",
        "    // --- 6. Copy results back and analyze ---\n",
        "    cudaMemcpy(h_output, d_output, N_chains * sizeof(float), cudaMemcpyDeviceToHost);\n",
        "\n",
        "    float posterior_mean = 0.0f;\n",
        "    for (int i = 0; i < N_chains; ++i)\n",
        "    {\n",
        "        posterior_mean += h_output[i];\n",
        "    }\n",
        "    posterior_mean /= N_chains;\n",
        "\n",
        "    printf(\"\\n--- Analysis ---\\n\");\n",
        "    printf(\"Posterior Mean of mu (from %d samples): %f\\n\", N_chains, posterior_mean);\n",
        "    printf(\"True Mean of mu was: %f\\n\", true_mu);\n",
        "\n",
        "    // --- 7. Cleanup ---\n",
        "    cudaFree(d_data);\n",
        "    cudaFree(d_output);\n",
        "    cudaFree(d_states);\n",
        "    free(h_data);\n",
        "    free(h_output);\n",
        "\n",
        "    return 0;\n",
        "}\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9K0_hSD4hjf8",
        "outputId": "813dbdf8-8325-436b-ae25-17c359be1ab0"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing mcmc_sampler.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc mcmc_sampler.cu -o mcmc_sampler \\\n",
        "    -gencode arch=compute_75,code=sm_75 \\\n",
        "    -gencode arch=compute_80,code=sm_80 \\\n",
        "    -gencode arch=compute_80,code=compute_80 \\\n",
        "    -lcurand"
      ],
      "metadata": {
        "id": "-xerbbBlhp0Q"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!time ./mcmc_sampler"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gQY3ZQKwkvDz",
        "outputId": "8ad07be3-2068-4745-d4a3-0666639f3f52"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Launching 16384 parallel MCMC chains...\n",
            "MCMC simulation finished.\n",
            "\n",
            "--- Analysis ---\n",
            "Posterior Mean of mu (from 16384 samples): 9.948833\n",
            "True Mean of mu was: 10.000000\n",
            "\n",
            "real\t0m1.567s\n",
            "user\t0m1.432s\n",
            "sys\t0m0.132s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### C++ (CPU)"
      ],
      "metadata": {
        "id": "lhZXBZowjORT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile mcmc_sampler_cpu.cpp\n",
        "// File: mcmc_sampler_cpu.cpp\n",
        "#include <iostream>\n",
        "#include <vector>\n",
        "#include <random>\n",
        "#include <cmath>   // For log, exp, pow, sqrt\n",
        "#include <numeric> // For std::accumulate\n",
        "#include <chrono>  // For seeding the random number generator\n",
        "\n",
        "// --- Helper function to compute the log posterior on the CPU ---\n",
        "// This is the direct equivalent of the __device__ function.\n",
        "float log_posterior(float mu, const std::vector<float> &data, float sigma2, float mu0, float tau2_0)\n",
        "{\n",
        "    // 1. Calculate log prior: log( N(mu | mu0, tau2_0) )\n",
        "    float log_prior = -0.5f * std::pow(mu - mu0, 2) / tau2_0;\n",
        "\n",
        "    // 2. Calculate log likelihood: log( N(data | mu, sigma2) )\n",
        "    float log_likelihood = 0.0f;\n",
        "    for (float val : data)\n",
        "    {\n",
        "        log_likelihood += -0.5f * std::pow(val - mu, 2) / sigma2;\n",
        "    }\n",
        "\n",
        "    return log_prior + log_likelihood;\n",
        "}\n",
        "\n",
        "// --- The MCMC simulation function for the CPU ---\n",
        "// This function replaces the CUDA kernel. It loops through each chain serially.\n",
        "void run_mcmc_cpu(std::vector<float> &output, const std::vector<float> &data,\n",
        "                  int N_chains, int N_iters, int N_burn_in,\n",
        "                  float sigma2, float mu0, float tau2_0, float prop_sigma)\n",
        "{\n",
        "\n",
        "    // Setup a high-quality random number generator for the CPU\n",
        "    unsigned seed = std::chrono::high_resolution_clock::now().time_since_epoch().count();\n",
        "    std::mt19937 generator(seed);\n",
        "    std::normal_distribution<float> prop_dist(0.0f, prop_sigma);\n",
        "    std::uniform_real_distribution<float> uniform_dist(0.0f, 1.0f);\n",
        "\n",
        "    // Loop over each chain (this was done in parallel on the GPU)\n",
        "    for (int chain_idx = 0; chain_idx < N_chains; ++chain_idx)\n",
        "    {\n",
        "        float current_mu = mu0; // Start each chain at the prior mean\n",
        "\n",
        "        // Main MCMC loop for this specific chain\n",
        "        for (int i = 0; i < N_iters + N_burn_in; ++i)\n",
        "        {\n",
        "            // Propose a new mu using the normal distribution\n",
        "            float proposed_mu = current_mu + prop_dist(generator);\n",
        "\n",
        "            // Calculate log posterior for current and proposed mu\n",
        "            float log_post_current = log_posterior(current_mu, data, sigma2, mu0, tau2_0);\n",
        "            float log_post_proposed = log_posterior(proposed_mu, data, sigma2, mu0, tau2_0);\n",
        "\n",
        "            // Acceptance check in log-space\n",
        "            float log_alpha = std::min(0.0f, log_post_proposed - log_post_current);\n",
        "            if (std::log(uniform_dist(generator)) < log_alpha)\n",
        "            {\n",
        "                current_mu = proposed_mu;\n",
        "            }\n",
        "        }\n",
        "\n",
        "        // After burn-in and iterations, store the final sample of the chain\n",
        "        output[chain_idx] = current_mu;\n",
        "    }\n",
        "}\n",
        "\n",
        "// --- The Main Host Function ---\n",
        "int main()\n",
        "{\n",
        "    // --- 1. Problem Setup ---\n",
        "    int N_data = 1000;\n",
        "    int N_chains = 1024 * 16; // Number of MCMC chains to run (16,384)\n",
        "    int N_iters = 2000;\n",
        "    int N_burn_in = 500;\n",
        "\n",
        "    float true_mu = 10.0f;\n",
        "    float sigma2 = 4.0f;\n",
        "\n",
        "    float mu0 = 0.0f;\n",
        "    float tau2_0 = 100.0f;\n",
        "\n",
        "    float prop_sigma = 1.0f;\n",
        "\n",
        "    // --- 2. Host-side Data Generation and Memory Allocation ---\n",
        "    // Using std::vector for automatic memory management\n",
        "    std::vector<float> h_data(N_data);\n",
        "    std::vector<float> h_output(N_chains);\n",
        "\n",
        "    // Generate synthetic data using C++'s <random> library\n",
        "    unsigned seed = std::chrono::high_resolution_clock::now().time_since_epoch().count();\n",
        "    std::mt19937 generator(seed);\n",
        "    std::normal_distribution<float> data_dist(true_mu, std::sqrt(sigma2));\n",
        "\n",
        "    for (int i = 0; i < N_data; ++i)\n",
        "    {\n",
        "        h_data[i] = data_dist(generator);\n",
        "    }\n",
        "\n",
        "    // --- 5. Launch the MCMC Simulation on the CPU ---\n",
        "    printf(\"Launching %d serial MCMC chains on the CPU...\\n\", N_chains);\n",
        "    run_mcmc_cpu(h_output, h_data, N_chains, N_iters, N_burn_in,\n",
        "                 sigma2, mu0, tau2_0, prop_sigma);\n",
        "    printf(\"MCMC simulation finished.\\n\");\n",
        "\n",
        "    // --- 6. Analyze results ---\n",
        "    // Use std::accumulate for a clean and efficient sum\n",
        "    float posterior_mean = std::accumulate(h_output.begin(), h_output.end(), 0.0f);\n",
        "    posterior_mean /= N_chains;\n",
        "\n",
        "    printf(\"\\n--- Analysis ---\\n\");\n",
        "    printf(\"Posterior Mean of mu (from %d samples): %f\\n\", N_chains, posterior_mean);\n",
        "    printf(\"True Mean of mu was: %f\\n\", true_mu);\n",
        "\n",
        "    // --- 7. Cleanup ---\n",
        "    // No manual free/delete needed thanks to std::vector!\n",
        "\n",
        "    return 0;\n",
        "}\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-ilu__jujREW",
        "outputId": "f16858e3-2d1a-4ccc-a9f7-55ffe068d0c9"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing mcmc_sampler_cpu.cpp\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!g++ -std=c++11 -O3 mcmc_sampler_cpu.cpp -o mcmc_sampler_cpu"
      ],
      "metadata": {
        "id": "X0fQjIDpjbXM"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!time ./mcmc_sampler_cpu"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "taWF39ULjuro",
        "outputId": "512e60ca-d021-4d31-ee0f-18137756e6b2"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Launching 16384 serial MCMC chains on the CPU...\n",
            "MCMC simulation finished.\n",
            "\n",
            "--- Analysis ---\n",
            "Posterior Mean of mu (from 16384 samples): 10.054132\n",
            "True Mean of mu was: 10.000000\n",
            "\n",
            "real\t6m48.670s\n",
            "user\t6m48.557s\n",
            "sys\t0m0.050s\n"
          ]
        }
      ]
    }
  ]
}